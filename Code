import torch
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import kneighbors_graph
from torch_geometric.data import Data
from sklearn.model_selection import KFoldS
from torch_geometric.nn import SAGEConv
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from catboost import CatBoostRegressor
import pandas as pd

# Define columns for features and target
continuous_columns = ['AGE', 'WT', 'HT', 'AMT', 'RATE', 'TIME']
categorical_columns = ['M1F2', 'A1V2']
target_column = 'CP'

# Load data
data = pd.read_csv('/content/Learncombined.csv')  # replace with the actual file path

# Standardize continuous features
scaler = StandardScaler()
continuous_features = scaler.fit_transform(data[continuous_columns])

# One-hot encode categorical features
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
categorical_features = encoder.fit_transform(data[categorical_columns])

# Combine continuous and categorical features
node_features = torch.tensor(
    np.concatenate([continuous_features, categorical_features], axis=1),
    dtype=torch.float
)

# Prepare target variable (CP)
target = torch.tensor(data[target_column].values, dtype=torch.float).view(-1, 1)

# Construct edge index using k-nearest neighbors graph (within each @ID group)
def construct_edges_per_subject(data, k_neighbors=5):
    edge_indices = []
    ids = data['@ID'].unique()

    for subject_id in ids:
        subject_data = data[data['@ID'] == subject_id]
        subject_features = np.concatenate([continuous_features, categorical_features], axis=1)[subject_data.index]
        adjacency_matrix = kneighbors_graph(subject_features, k_neighbors, mode='connectivity', include_self=True)
        edge_index = np.array(adjacency_matrix.nonzero(), dtype=np.int64)
        edge_indices.append(edge_index)

    return torch.tensor(np.hstack(edge_indices), dtype=torch.long)

edge_index = construct_edges_per_subject(data)

# Create the graph data for PyTorch Geometric
graph_data = Data(x=node_features, edge_index=edge_index, y=target)

# Define the Enhanced GraphSAGE model with more layers
class EnhancedGraphSAGEModel(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout_rate):
        super(EnhancedGraphSAGEModel, self).__init__()

        # Define multiple SAGEConv layers dynamically
        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()

        # First layer
        self.convs.append(SAGEConv(in_channels, hidden_channels))
        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))

        # Hidden layers
        for _ in range(num_layers - 1):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels))
            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))

        # Final fully connected layer for output
        self.fc = torch.nn.Linear(hidden_channels, out_channels)
        self.dropout_rate = dropout_rate

    def forward(self, x, edge_index):
        for conv, bn in zip(self.convs, self.bns):
            x = conv(x, edge_index)
            x = bn(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout_rate, training=self.training)

        x = self.fc(x)
        return x

# K-Fold Cross-Validation with CatBoost
def cross_validate_with_catboost(graph_data, n_splits=5, num_epochs=400, learning_rate=0.001, weight_decay=1e-4):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    metrics = {'mse': [], 'rmse': [], 'mae': [], 'r2': []}
    train_losses_all = []
    val_losses_all = []

    for train_index, test_index in kf.split(range(graph_data.num_nodes)):
        train_mask = torch.tensor(train_index, dtype=torch.long)
        test_mask = torch.tensor(test_index, dtype=torch.long)

        # Train Enhanced GNN model
        model = EnhancedGraphSAGEModel(
            in_channels=graph_data.x.shape[1],
            hidden_channels=512,  # Increased hidden units for more capacity
            out_channels=1,
            num_layers=5,  # Increased number of layers to 5
            dropout_rate=0.4  # Increased dropout for regularization
        )

        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        criterion = torch.nn.MSELoss()

        train_losses = []
        val_losses = []

        for epoch in range(num_epochs):
            model.train()
            optimizer.zero_grad()
            out = model(graph_data.x[train_mask], graph_data.edge_index)
            loss = criterion(out, graph_data.y[train_mask])
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())

            # Validate the model
            model.eval()
            with torch.no_grad():
                val_out = model(graph_data.x[test_mask], graph_data.edge_index)
                val_loss = criterion(val_out, graph_data.y[test_mask])
                val_losses.append(val_loss.item())

        # Generate embeddings using GNN for CatBoost
        with torch.no_grad():
            embeddings_train = model(graph_data.x[train_mask], graph_data.edge_index).cpu().numpy()
            embeddings_test = model(graph_data.x[test_mask], graph_data.edge_index).cpu().numpy()

        y_train = graph_data.y[train_mask].cpu().numpy().flatten()  # Flatten to 1D
        y_test = graph_data.y[test_mask].cpu().numpy().flatten()    # Flatten to 1D

        # Train the CatBoost Regressor
        catboost_model = CatBoostRegressor(iterations=500, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
        catboost_model.fit(embeddings_train, y_train)

        # Predictions
        preds = catboost_model.predict(embeddings_test)

        # Calculate metrics
        mse = mean_squared_error(y_test, preds)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, preds)
        r2 = r2_score(y_test, preds)

        # Store metrics
        metrics['mse'].append(mse)
        metrics['rmse'].append(rmse)
        metrics['mae'].append(mae)
        metrics['r2'].append(r2)

        train_losses_all.append(train_losses)
        val_losses_all.append(val_losses)

    # Average metrics across folds
    avg_metrics = {key: np.mean(value) for key, value in metrics.items()}

    # Plot learning curves for the last fold
    plt.figure(figsize=(12, 5))
    plt.plot(train_losses_all[-1], label='Train Loss')
    plt.plot(val_losses_all[-1], label='Validation Loss')
    plt.title('Learning Curves for Last Fold')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Bar graph for metrics
    plt.figure(figsize=(10, 5))
    colors = sns.color_palette("viridis", len(avg_metrics))  # Generate a color palette
    plt.bar(avg_metrics.keys(), avg_metrics.values(), color=colors)
    plt.title('Average Metrics across K-Folds')
    plt.ylabel('Score')
    plt.xticks(rotation=45)
    plt.show()

    return avg_metrics

# Main execution
if __name__ == "__main__":
    avg_metrics = cross_validate_with_catboost(graph_data)

    print("Average Metrics:")
    for key, value in avg_metrics.items():
        print(f"Average Test {key.capitalize()}: {value}")
